= Kubernetes MultiNode cluster on VirtualBox

A step by step description of bringing up kubernetes cluster using VirtualBox 6.0 with ubuntu 18.04 server.

== VirtualBox setup

Create two VMs k8s-master and k8s-s1 as shown in below diagram. 

[ditaa]
----
    +----------------+                       +----------------+
    |                |                       |                |
    |   K8s-master   |                       |   K8s-s1       |
    |                |                       |                |
    | IF2        IF1 |                       | IF1        IF2 |
    +--+----------+--+                       +---+---------+--+
       |          |                              |         |
       |          |       Host only Network      |         |
       |          +------------------------------+         |
      NAT                  192.168.56.0/24               NAT


                          VirtualBox VM setup
----

== VM hardware configuration.

* Kubernetes master/controller node hardware requirement
[options="k8s-master",cols="1,1"]
|===
|Component   | Value (min) 
//-------------
|Processor   | 2 core   
|RAM         | 4GB   
|Network Adapter-1      | Host only network   
|Network Adapter-2      | NAT   
|===

* kubernetes worker/slave node hardware requirement
[options="k8s-master",cols="1,1"]
|===
|Component   | Value (min)
//-------------
|Processor   | 1 core   
|RAM         | 2GB   
|Network Adapter-1      | Host only network   
|Network Adapter-2      | NAT   
|===


== Prepare Servers
	
Execute the below step in both the VMs.

=== Setup network.

* Shutdown the VMs.
* Modify VM network, with Adapter 1 for Host Only Network and Adapter 2 for NAT.
* Start VM.
* Execute below command and note down the interfaces (generally for Virtual box VMs network interface will start with enp0s).
+
[source, shell,options="nowrap"]
----
$ sudo ifconfig -a
----

* Setup static network. 
+
Open file `/etc/netplan/50-cloud-init.yaml` and add below configuration.
+
.*k8s-master*
[source, yaml]
----
network:
 ethernets:
  enp0s3:
   addresses: [192.168.56.10/24]
   dhcp4: false
  enp0s8:
   addresses: []
   dhcp4: true
 version: 2
----
+
.*k8s-s1*
[source, yaml]
----
network:
 ethernets:
  enp0s3:
   addresses: [192.168.56.12/24]
   dhcp4: false
  enp0s8:
   addresses: []
   dhcp4: true
 version: 2
----

* Update `/etc/hosts` such that command `hostname -i` outputs ip which is routable from other VM.
+
.example(master)
----
192.168.56.10 k8s-master
192.168.56.12 k8s-s1
----
+
.example(slave)
----
192.168.56.10 k8s-master
192.168.56.12 k8s-s1
----
+
NOTE: This step is required since we've chosen host-only adapter and our default route is provider by NAT
network, if we choose Bridge network this step can be skipped.

* Reboot VMs.

//tag::packageUpdate[]
* Update Packages.
** Update apt sources.
+
[source, shell,options="nowrap"]
----
$ sudo apt update
----	
** Install packages
+
[source, shell,options="nowrap"]
----	
$ sudo apt upgrade
----
//end::packageUpdate[]

=== Install docker
//tag::installDocker[]
* Install utility packages
+
[source%autofit, shell,options="nowrap"]
----
$ sudo apt install apt-transport-https ca-certificates curl software-properties-common
----
* Add GPG key
+
[source, shell,options="nowrap"]
----
$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
----
* Add docker repository to apt sources.
+
[source%autofit, shell,options="nowrap"]
----
$ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable"
----
* Update apt sources and install docker
+
[source, shell,options="nowrap"]
----
$ sudo apt update ; sudo apt install docker-ce
----
* Verify docker service is started.
+
[source, shell,options="nowrap"]
----
$ sudo systemctl status docker
----
//end::installDocker[]

== Install and configure kubernetes.

=== Prepare server

//tag::prepareServer[]
Execute the below steps on both the VMs.

. Add Kubernetes source gpg.
+
[source, shell,options="nowrap"]
----
$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
----
. Next add kubernetes repository 
+
[source, shell,options="nowrap"]
----
$ sudo apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"
----
NOTE: Browse http://apt.kubernetes.io/ and search for the corresponding ubuntu version, since there was no kubernetes-bionic, installing kubernetes-xenial in my case.	
. Install `kubeadm`
+
[source, shell,options="nowrap"]
----
$ sudo apt update; sudo apt install kubeadm kubelet kubectl
----
. Turn off swap
+
[source, shell,options="nowrap"]
----
$ sudo swapoff -a
----	
. Turn off swap permanently in `/etc/fstab` by executing belo command.
+
----
$ sudo sed -i '/swap/s/^\(.*\)$/#\1/g' /etc/fstab
----
+
IMPORTANT:  If 4 and 5 steps not followed, kubelet service will not start.

. Add private ip to kubernetes arguments
+
While installing `kubeadm` you might get error related to port forwarding if you are using private IP which is true in our case. To resolve this issue,
Create file `/etc/default/kubelet` and following content in the file.
+
[sh]
```
KUBELET_EXTRA_ARGS=--node-ip=192.168.56.10
```

. Use systemd for cgroups.
+
Create file `/etc/docker/daemon.json` and add following content.
+
[json]
```
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}

```

. Reload configuration and restart docker service.
+
[sh]
```
sudo systemctl daemon-reload
sudo systemctl restart docker
```
//end::prepareServer[]
. Reboot VMs.

=== Configure Master VM.

. Initialize kubernetes master node by executing below command.
+
[source%autofit, shell,options="nowrap"]
----
$ kubeadm init --apiserver-advertise-address=192.168.56.9 \
   --apiserver-cert-extra-sans=192.168.56.9  \
   --pod-network-cidr=10.244.0.0/16 -v 1
----
+
WARNING: If decide to change pod network remember to change the pod network in `flannel.yaml`
while setting up pod network in below step. 
+
. As output of above command shows to execute below command, run below command to update auth details
for `kubectl` command.
+
[source, shell,options="nowrap"]
----
$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
----	

. Note down (copy to textpad) the `kubeadm` join command which needs to be executed on slave nodes to join to the cluster, we not run the command now, instead we will execute once dashboard is installed as we need dashboard to be installed on master node.

. Install pod network.
+
.. If you are using bridge network which has default route then you can directly install flannel using below command.
However if you had given host-only adapter IP address for API server. then skip this step and follow next steps.
+
[source%autofit, shell,options="nowrap"]
----
$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
----

.. Download flannel.yaml
+
[source%autofit,sh]
----
$ wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
----

.. Open `kube-flannel.yaml` file search for daemonset kind and add additional arugment `--iface=<host-only-adaper-interface>`, note there are
multiple daemonset definitions, either update all of them or execute `kubectl get nodes --show-labels` and get the arch and update corresponding
arch daemonset only.
+
[source,yaml,]
----
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.11.0-amd64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        - --iface=enp0s3 // <1>
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
----
<1> Added my host-only adapter here

.. if you had changed pod network while executing `kubeadm init`, replace `10.244.0.0/16`
which chosen pod network above in `net-conf.json` section.

.. Now Create pod netwrok by applying updated yaml file
+
----
$ kubectl apply -f kube-flannel.yaml
----

. Verify all necessary pods are started
+
[source, shell,options="nowrap"]
----
$ kubectl get pods --all-namespaces
----
+
.output:
[source%autofit, shell,options="nowrap"]
----	
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE
kube-system   coredns-86c58d9df4-c68gd             1/1     Running   0          6m41s
kube-system   coredns-86c58d9df4-q5bht             1/1     Running   0          6m41s
kube-system   etcd-k8s-master                      1/1     Running   0          6m6s
kube-system   kube-apiserver-k8s-master            1/1     Running   0          5m59s
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          5m56s
kube-system   kube-flannel-ds-amd64-stb29          1/1     Running   0          49s
kube-system   kube-proxy-882ms                     1/1     Running   0          6m41s
kube-system   kube-scheduler-k8s-master            1/1     Running   0          5m54s
----

=== Configure Kubernetes Dashboad.

==== Install Dashboard

. Deploy dashboard.
+
[source%autofit, shell,options="nowrap"]
----
$ kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended/kubernetes-dashboard.yaml
----
. Wait till dashboard pod is running.
+
[source, shell,options="nowrap"]
----
$ kubectl get pods --all-namespaces
----
+
.output:
[source%autofit, shell,options="nowrap"]
----	
NAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE
kube-system   coredns-86c58d9df4-c68gd               1/1     Running   0          11m
kube-system   coredns-86c58d9df4-q5bht               1/1     Running   0          11m
kube-system   etcd-k8s-master                        1/1     Running   0          10m
kube-system   kube-apiserver-k8s-master              1/1     Running   0          10m
kube-system   kube-controller-manager-k8s-master     1/1     Running   0          10m
kube-system   kube-flannel-ds-amd64-stb29            1/1     Running   0          5m18s
kube-system   kube-proxy-882ms                       1/1     Running   0          11m
kube-system   kube-scheduler-k8s-master              1/1     Running   0          10m
kube-system   kubernetes-dashboard-57df4db6b-5phx2   1/1     Running   0          35s
----
. By default dashboard cannot be accessed from outside the VM, if you are using ubuntu desktop you can run below command and 
access the dashboard using proxy at url  `http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/`
+
[source,shell,options="nowrap"]
----
$ kubectl proxy
----
.. However if you want dashboard be accessed from external ip editing kubernetes-dashboard service and changing type from ClusterIP to NodePort.
+
[source, shell,options="nowrap"]
----
$ kubectl edit service kubernetes-dashboard -n kube-system
----
The file content should something as shown below 
+
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2019-01-21T18:06:35Z"
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
  resourceVersion: "1885"
  selfLink: /api/v1/namespaces/kube-system/services/kubernetes-dashboard
  uid: 4a2d8f61-1da7-11e9-9d52-080027aba7cb
spec:
  clusterIP: 10.110.253.116
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 32608
    port: 443
    protocol: TCP
    targetPort: 8443
  selector:
    k8s-app: kubernetes-dashboard
  sessionAffinity: None
  type: ClusterIP # <1>
status:
  loadBalancer: {}
----
<1> Replace *ClusterIP* with *NodePort*
. Execute the below command and note down the port
+
[source, shell,options="nowrap"]
----	
$ kubectl get service --all-namespaces
----
+
.output:
+
[source%autofit, shell,options="nowrap"]
----	
NAMESPACE     NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE
default       kubernetes             ClusterIP   10.96.0.1        <none>        443/TCP         21m
kube-system   kube-dns               ClusterIP   10.96.0.10       <none>        53/UDP,53/TCP   21m
kube-system   kubernetes-dashboard   NodePort    10.110.253.116   <none>        443:32608/TCP   10m
----
+
NOTE: The dashboard port is *32608* in my case.
	
. Now we can access dashboard at URL. https://192.168.56.10:32608
+
CAUTION: Port will be dynamically generated and port should be replaced from step 5.

==== Create service Account and access dashboard.

. Create a service account
+
[source, shell,options="nowrap"]
----
$ kubectl create serviceaccount admin-user -n kube-system
----
Verification : Below command should list the  admin-user account
+
[source, shell,options="nowrap"]
----
$ kubectl get serviceaccount --all-namespaces
----
. Create Cluster Role binding for the user.
+
[source%autofit,sh,options="nowrap"]
----
$  kubectl create clusterrolebinding admin-user -n kube-system \
     --clusterrole=cluster-admin \
     --serviceaccount=kube-system:admin-user
----	
. Generate the Bearer Token to access Dashboard
+
[source, shell,options="nowrap"]
----
$ kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | \
      grep admin-user | \
      awk '{print $1}')
----	
+
.output:
[source%autofit, shell,options="nowrap"]
----
Name:         admin-user-token-4nwz2
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: admin-user
              kubernetes.io/service-account.uid: a1e3ca50-1dab-11e9-9d52-080027aba7cb

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLTRud3oyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJhMWUzY2E1MC0xZGFiLTExZTktOWQ1Mi0wODAwMjdhYmE3Y2IiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.YHRkrY1dPsrf1N4LU6qGqCPPl617faeBbHelJAdWXD3TvvZMYnQdMvZuWtFZjVMxXPdgXDud17eCffDXBg5bRAs1sxd7B37IbXVULrYFoMR-B0MjOa3eLx1edO_gvE6ZqpyPpdWxC0hWYI0P9cQ78oyZEZ0RDNctTus0qRpVrHpP5ZIMhfRPknV8zxxF-zGf8Xg8ni1NxUOHHB-DYO1T6gd4v65JgD2ohLS4N9rLpq_MrA7nc13R4sE6zDIgYi5V7kZYz0Zx72qAaV4oOGMDTr0FPP7q3m9SrH8uO3UOUe9tkp_ce8-7V9hJW8AbPHu3rLNBw2dOGnOk59yNe3jv5w
----
+
Copy the token and paste it into token field in the URL to Dashboard and login to dashboard.

=== Configuration slave node.

. Now go to slave node and execute the join command previously saved when you were executing kubeadm on master.
+
[source%autofit, shell,options="nowrap"]
----
$  kubeadm join 192.168.56.10:6443 --token t0j1zi.v5lojsnpjh9r0rbn \
       --discovery-token-ca-cert-hash sha256:40b1142d9002003ab5b085776b8b8cba4a41ceaafab06429c49eaedc2b2939fa
----
+
NOTE: The above command is sample, the values are dynamically generated.
	
. Now go back to master and execute the below command, you should be able to see slave node added.
+
[source, shell,options="nowrap"]
----
$ kubectl get nodes
----
+	
.output:
[source%autofit, shell,options="nowrap"]
----	
NAME         STATUS   ROLES    AGE     VERSION
k8s-master   Ready    master   56m     v1.13.2
k8s-s1       Ready    <none>   2m49s   v1.13.2
----


=== Install Helm

Helm package manager consists of helm client and tiller component which runs as pod on k8s. We will install helm client on our master server itself.

==== Install helm client.

. Install helm client using snap on master server using below command.
+
[sh]
```
$ sudo snap install helm --classic
```

. Verify helm client is running by executing command `helm version`, it should give version details for client and for server side should return error until we install tiller.

==== Install Tiller.

Before installing tiller we need to create RBAC for the tiller pod.

. Create service account for tiller.
+
[sh]
```
$ kubectl create serviceaccount -n kube-system tiller
```
. Provide cluster role for the above service account.
+
[sh]
```
$ kubectl create clusterrolebinding tiller-cluster-rule \
   --clusterrole cluster-admin \
   --serviceaccount=kube-system:tiller
```

. Now create tiller using the service account 
+
[sh]
```
helm init --service-account tiller
```

. Wait until tiller pod comes up.
+
[sh]
```
kubectl get po --all-namespaces --watch
```
. Once tiller pod is up, run `helm version`, the output provide version details for both client and server.

=== Install Ingress controller

Optionally if you want to expose Any API/web application install ingress controllers such as NGINX, traefik etc.

====== Install Traefik Ingress controller.

. Execute below helm command to install traefik ingress.
+
[sh]
```
$ helm install --name treafik-ingress --namespace kube-system  --set dashboard.enabled=true --set rbac.enabled=true stable/traefik
```

====== Installing NGINX ingress controller.

. Execute below helm command to install nginx ingress.
+
[sh]
```
$ helm install stable/nginx-ingress --name nginx-ingress --set rbac.create=true
```

== Troubleshoot.

https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/ - DNS not resolving

https://kubernetes.io/docs/setup/independent/troubleshooting-kubeadm/ - kubeadm

==== References

https://github.com/helm/charts/tree/master/stable/traefik - traefik

https://github.com/helm/charts/tree/master/stable/nginx-ingress - nginx

https://kubernetes.io/docs/concepts/services-networking/ingress/ - ingress controller concepts

